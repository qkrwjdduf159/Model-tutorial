{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "위의 Normal Equation에서는 역행렬을 구할 수 있다면 정확한 해를 구할 수 있었다.\n",
    "\n",
    "하지만 역행렬이 없거나, 데이터의 개수가 많으면 연산량이 급격히 증가하는 문제점이 있었다.\n",
    "\n",
    "그래서 우리는 비용 최소화를 위해서 Gradient Descent를 사용하게 됩니다.\n",
    "\n",
    "Gradient Descent는 Normal Equation에서 말하는 MSE를 최소화 하기 위해서 RSS를 도입한다.\n",
    "\n",
    "## RSS(Residual Sum of Squares) 잔차제곱합\n",
    "\n",
    "RSS = $\\sum_{i=1}^{n} (y_i - f(x_i))^2$\n",
    "\n",
    "$ y_i : i 번째 종속변수의 값$\n",
    "\n",
    "$ x_i : i 번째 독립변수의 값$\n",
    "\n",
    "단순 선형 회귀모델의\n",
    "\n",
    "$y_i = a + bx_i + \\epsilon_i $ 에서는\n",
    "\n",
    "a, b는 계수\n",
    "\n",
    "y는 종속변수, x는 독립변수, $\\epsilon$는 오차항인데\n",
    "\n",
    "RSS(잔차제곱합)은 $\\epsilon_i$의 추정값의 제곱합이다.\n",
    "\n",
    "다시 작성해보면\n",
    "\n",
    "RSS = $\\sum_{i=1}^{n}(\\epsilon_i)^2$ = $\\sum_{i=1}^{n}(y_i - (\\alpha + \\beta x_i))^2$\n",
    "\n",
    "$\\alpha$ 는 상수항 a는 추정치\n",
    "\n",
    "$\\beta$ 는 회귀계수 b는 추정치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient descent 방법의 직관적 이해\n",
    "\n",
    "\n",
    "자신이 한치앞도 잘 안보이는 울창한 밀림에 있을 때 산 정상으로 가기 위한 방법은 간단합니다. 비록 실제 산 정상이 어디에 있는지는 모르지만 현재 위치에서 가장 경사가 가파른 방향으로 산을 오르다 보면 언젠가는 산 정상에 다다르게 될 것입니다.\n",
    "\n",
    "\n",
    "\n",
    "또는 이와 반대로 깊은 골짜기를 찾고 싶을 때에는 가장 가파른 내리막 방향으로 산을 내려가면 될 것입니다.\n",
    "\n",
    "\n",
    "\n",
    "이와 같이 어떤 함수의 극대점을 찾기 위해 현재 위치에서의 gradient 방향으로 이동해 가는 방법을 gradient ascent 방법, 극소점을 찾기 위해 gradient 반대 방향으로 이동해 가는 방법을 gradient descent 방법이라 부릅니다.\n",
    "\n",
    "- 참고자료 : https://darkpgmr.tistory.com/133"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We show Gradient Descent Proof\n",
    "\n",
    "우리는 잔차 제곱합의 값을 최소화 해야한다.\n",
    "\n",
    "x 는 바꿀수가 없기 때문에 우리는 $\\beta$의 값과 $\\alpha$의 값의 변화로 RSS를 줄여야한다.\n",
    "\n",
    "\n",
    "독립변수 1개 종속변수 1개를 기준으로 경사하강법을 설명하도록 하겠습니다.\n",
    "\n",
    "$\\hat{Y} = WX$\n",
    "\n",
    "여기서 cost를 이렇게 나타내도록 하겠습니다.\n",
    "\n",
    "cost(W) = $\\frac{1}{2} \\sum_{d} (Y - WX)^2$\n",
    "\n",
    "우리는 cost함수를 최소화 하기 위해서 W의 값을 cost의 미분값을 이용해서 변경을 해준다.\n",
    "\n",
    "$\\frac{\\partial}{\\partial W_i}cost(W)$ = $\\frac{\\partial}{\\partial W_i} (\\frac{1}{2} \\sum_{i=1}^{n}(WX - Y)^2)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial W_i}cost(W)$ = $\\frac{1}{2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial W_i} ((WX - Y)^2)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial W_i}cost(W)$ = $\\frac{1}{2} \\sum_{i=1}^{n} 2(WX - Y) \\frac{\\partial}{\\partial W_i} ((WX - Y)^2)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial W_i}cost(W)$ = $\\sum_{i=1}^{n} ((WX - Y)^2) X_i$\n",
    "\n",
    "즉, 미분을 계산하는 계산값이 원래 값에 X_i의 값을 곱해주는 방식으로 경사 하강법이 전개가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'http://mccormickml.com/assets/GradientDescent/ThetaZeroDerivation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://2.bp.blogspot.com/-lKv7rr5crQk/XE6nkO_veNI/AAAAAAAAA3Q/QRgFAe2TK3I9A3uuBnUBzAQX2tEXHtkOgCEwYBhgL/s1600/%25E1%2584%2589%25E1%2585%25B3%25E1%2584%258F%25E1%2585%25B3%25E1%2584%2585%25E1%2585%25B5%25E1%2586%25AB%25E1%2584%2589%25E1%2585%25A3%25E1%2586%25BA%2B2019-01-25%2B%25E1%2584%258B%25E1%2585%25A9%25E1%2584%2592%25E1%2585%25AE%2B4.59.37.png'>\n",
    "\n",
    "- 참고자료 : https://angeloyeo.github.io/2020/08/16/gradient_descent.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
