{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Yolo v1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMoEoQLh79z/A/3kNykmE+C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AQH0ASt_k9ZS"},"source":["## 참고한 자료\n","\n","https://deep-learning-study.tistory.com/402"]},{"cell_type":"markdown","metadata":{"id":"Q1UaPWd_jq46"},"source":["## 간단한 용어 설명"]},{"cell_type":"markdown","metadata":{"id":"ZsVKLyTLju_g"},"source":["### IoU(Intersection over Union)\n","\n","IoU는 object detector의 정확도를 측정하는데 이용되는 평가 지표입니다.\n","\n","IoU를 적용하기 위해서는 두 가지가 필요합니다.\n","\n","1. ground-truth bounding boxes(testing set에서 object 위치를 labeling 한 것)\n","\n","2. predicted bounding boxes(model이 출력한 object 위치 예측값)\n"]},{"cell_type":"markdown","metadata":{"id":"EdP5j1ivkKOJ"},"source":["<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwNXOK%2FbtqSpGVHmHc%2FKbsxRBSs6KymYB3PkEny21%2Fimg.png'>"]},{"cell_type":"markdown","metadata":{"id":"RvrFYuQpkhZb"},"source":["- Area of Overlap은 predicted bounding box와 ground-truth bounding box가 겹치는 부분\n","- Area of Union은 predicted bounding box와 ground-truth bounding box를 둘러싸는 영역\n","\n","즉 overlap은 교집합 union은 합집합으로 이해하는게 제 입장에선 편했습니다(개인적인 생각)\n","\n","$\\frac{P(A \\cap B)}{P(A \\cup B)}$ 전체 예측과 실제 값과의 전체 bounding box 면적과 그 값들의 겹치는 부분의 비율이라고 이해했습니다.\n","\n","그러면 왜?? object detection에서는 이러한 평가기준을 사용하는 것인가?\n","\n","object detection에서는 정확하게 겹치는 부분을 찾아보기 어렵습니다. 또한 머신러닝처럼 accuracy나 rmse로 정확도를 측정하기 어렵기 때문입니다."]},{"cell_type":"markdown","metadata":{"id":"2Ik4EiRV_B_7"},"source":["## YOLO v1\n","\n","R-CNN의 object detection 모델과는 다르게 YOLO는 1-stage detector로 detection을 수행한다.\n","\n","YOLO는 DPM과 R-CNN 방법을 내포하고 있다."]},{"cell_type":"markdown","metadata":{"id":"WohDFvbWELmp"},"source":["- YOLO는 이미지로부터 한 번에 Class와 Bounding Box를 예측\n","- Inference가 굉장히 빠름\n","- 전체 이미지를 보고 Object detection을 수행하기 때문에 배경 오류가 적고 일반화 성능이 좋음\n","- 성능이 낮고(small object의 검출이 좋지 않음)\n"]},{"cell_type":"markdown","metadata":{"id":"-2lZbkyXEh1F"},"source":["<img src = 'https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbaWsjr%2FbtqVsxhJ8BD%2F54ZVdkz1YoNduB1bXkIyb1%2Fimg.png'>"]},{"cell_type":"markdown","metadata":{"id":"L0AuaHjoigsw"},"source":["## Bounding Box 예측\n","\n","1. 입력 이미지를 S X S grid로 분할합니다.\n","\n","2. 객체의 중심이 grid cell에 맞아 떨어지면 그 grid cell은 객체를 탐지했다고 표기합니다.\n","\n","3. 각 grid cell은 B개의 바운딩박스와 각 바운딩밗스에 대한 coonfidence score를 예측합니다.\n","\n","여기서 confidence score는 얼마나 자신있게 박스가 객체를 포함하고 있는지, 얼마나 정확하게 예측했는지를 반영한다.\n","\n","4. 각 바운딩 박스는 5개의 정보를 가지고 있습니다. x,y,w,h 와 confidence 입니다. x,y는 바운딩 박스의 중심을 나타내며 grid cell의 경계에 상대적인 값입니다. width, height는 전체 이미지에 상대적인 값입니다. 마지막으로 confidence는 예측된 박스와 진짜 박스 사이의 IoUㅇ입니다.\n","\n","5. 각 grid cell은 바운딩 박스 이외에도 class 확률을 예측합니다.\n","\n","최종적으로 예측값은 (S X S X (B * 5 + C)) 크기의 tensor를 갖습니다. 논문에서는 S = 7, B = 2, C = 20을 사용하여 7 X 7 X 30 tensor를 갖습니다.\n","\n","6. non-max suppression을 거쳐서 최종 바운딩 박스를 선정합니다.(NMS)"]},{"cell_type":"markdown","metadata":{"id":"XccgbfTNignc"},"source":[""]}]}