{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MaskRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UPMPCDkZhuZ3",
        "EvwpoiUJdKhA",
        "DU7KIN18dNHy"
      ],
      "machine_shape": "hm",
      "mount_file_id": "1QAad03Vhd3xys9z_WCrzuVbJUL6U47Wn",
      "authorship_tag": "ABX9TyM2lfPjWIuipur3/86okOyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qkrwjdduf159/Model-tutorial/blob/main/Model%20tutorial%20code/MaskRCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SStS257HZBwJ"
      },
      "source": [
        "출처: https://dacon.io/codeshare/3725?dtype=recent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcQm2fkiZGNw"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch  import nn, Tensor\n",
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import base64\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa_53qNpSC29",
        "outputId": "cd6efd1c-4684-4578-ea0a-045e43ef63a9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coco_eval.py   drive\t  __pycache__  transforms.py  vision\n",
            "coco_utils.py  engine.py  sample_data  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMPCDkZhuZ3"
      },
      "source": [
        "## 버전 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xdZkYLLhvnN",
        "outputId": "6ec9a300-be87-4f37-d42a-623bf3b8678b"
      },
      "source": [
        "print('torch :', torch.__version__)\n",
        "print('torchvision : ', torchvision.__version__)\n",
        "print('cv2 :', cv2.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch : 1.10.0+cu111\n",
            "torchvision :  0.11.1+cu111\n",
            "cv2 : 4.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwpoiUJdKhA"
      },
      "source": [
        "## 전체 데이터 json 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIBJ0FI2UIvj"
      },
      "source": [
        "# # # 이건 저의 개인적인 코랩이기 때문에 다르신 분들은 다시 변경하시기 바랍니다.\n",
        "# # ## train_PATH와 test_PATH만 변경하면 됩니다.\n",
        "# # # json 파일로 만들어서 분석이 더 용이하다고 생각되어 만든 것이니 더 좋은 의견 있으면 말씀해주시기 바랍니다.\n",
        "\n",
        "# ## Train\n",
        "# train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/train/annotation'\n",
        "# file_list = os.listdir(train_PATH)\n",
        "\n",
        "# train_files = []\n",
        "# for file in tqdm(file_list):\n",
        "#     dir = train_PATH + '/' + file\n",
        "#     json_list = glob(dir + '/' +'*.json')\n",
        "#     train_files.append(json_list)\n",
        "\n",
        "# train_json_list = []\n",
        "# for files in tqdm(train_files):\n",
        "#     for json_file in tqdm(files):\n",
        "#         train_json_list.append(json_file)\n",
        "\n",
        "# ## Test\n",
        "# test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/test/annotations'\n",
        "# file_list = os.listdir(test_PATH)\n",
        "\n",
        "# test_files = []\n",
        "# for file in tqdm(file_list):\n",
        "#     dir = test_PATH + '/' + file\n",
        "#     json_list = glob(dir + '/' + '*.json')\n",
        "#     test_files.append(json_list)\n",
        "\n",
        "# test_json_list = []\n",
        "# for files in tqdm(test_files):\n",
        "#     for json_file in tqdm(files):\n",
        "#         test_json_list.append(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU7KIN18dNHy"
      },
      "source": [
        "## Sample data에서 json 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYC1fY7fdCwt",
        "outputId": "a3091b0b-302e-461a-cbf5-8305e61cd9e6"
      },
      "source": [
        "## Train\n",
        "train_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train/annotation'\n",
        "file_list = os.listdir(train_PATH)\n",
        "\n",
        "train_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = train_PATH + '/' + file\n",
        "    json_list = glob(dir + '/' +'*.json')\n",
        "    train_files.append(json_list)\n",
        "\n",
        "train_json_list = []\n",
        "for files in tqdm(train_files):\n",
        "    for json_file in tqdm(files):\n",
        "        train_json_list.append(json_file)\n",
        "\n",
        "\n",
        "test_PATH = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/test/annotations'\n",
        "file_list = os.listdir(test_PATH)\n",
        "test_files = []\n",
        "for file in tqdm(file_list):\n",
        "    dir = test_PATH + '/' + file\n",
        "    json_list = glob(dir + '/' +'*.json')\n",
        "    test_files.append(json_list)\n",
        "\n",
        "test_json_list = []\n",
        "for files in tqdm(test_files):\n",
        "    for json_file in tqdm(files):\n",
        "        test_json_list.append(json_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 878.62it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 160328.34it/s]\n",
            "\n",
            "100%|██████████| 56/56 [00:00<00:00, 401506.02it/s]\n",
            "\n",
            "100%|██████████| 56/56 [00:00<00:00, 608500.06it/s]\n",
            "\n",
            "100%|██████████| 56/56 [00:00<00:00, 414252.25it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 161.93it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 526.38it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\n",
            "100%|██████████| 14/14 [00:00<00:00, 139147.53it/s]\n",
            "\n",
            "100%|██████████| 14/14 [00:00<00:00, 198379.24it/s]\n",
            "\n",
            "100%|██████████| 14/14 [00:00<00:00, 187604.65it/s]\n",
            "\n",
            "100%|██████████| 14/14 [00:00<00:00, 164023.06it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 178.89it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD3TaUlo1gZY"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms = None, mode = 'train', json_file = None):\n",
        "        self.mode = mode\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.json_file = json_file\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train':\n",
        "\n",
        "            annot = json_file[idx]\n",
        "            # Train의 경로로 이동한다.\n",
        "            # 나중에 진짜 데이터를 다룰 때 사용한다.\n",
        "            # PATH = root + '/' + 'train'\n",
        "\n",
        "            with open(annot, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "                \n",
        "            image_name = annot['images'][0]['file_name']\n",
        "            dir = image_name.split('_')[0]\n",
        "\n",
        "            image_PATH = self.root + '/' + 'train' + '/' +'image' +'/' + dir + '/' + image_name\n",
        "            \n",
        "            img = Image.open(image_PATH).convert('RGB')\n",
        "\n",
        "            boxes = []\n",
        "            segmentations = []\n",
        "            labels = []\n",
        "\n",
        "            for i in range(len(annot['annotations'])):\n",
        "\n",
        "                segmentation = annot['annotations'][i]['segmentation'][0]\n",
        "                bbox = annot['annotations'][i]['bbox']\n",
        "                label = torch.tensor(annot['annotations'][i]['category_id'], dtype = np.uint8)\n",
        "                xmin, ymin, width, height = bbox[0],bbox[1],bbox[2],bbox[3]\n",
        "                xmin, ymin, xmax, ymax = xmin, ymin, xmin + width, ymax + height\n",
        "                \n",
        "                boxes.append(torch.as_tensor([xmin, ymin, xmax, ymax], dtype = torch.float32))\n",
        "                segmentations.append([segmentation])\n",
        "                labels.append(label)\n",
        "\n",
        "            target = {}\n",
        "            target['boxes'] = boxes\n",
        "            target['labels'] = labels\n",
        "            target['segmentation'] = segmentations\n",
        "            target['image_id'] = torch.tensor(idx)\n",
        "\n",
        "            if self.transforms is not None:\n",
        "                img, target = self.transforms(img, target)\n",
        "\n",
        "            return img, target\n",
        "\n",
        "        if self.mode == 'test':\n",
        "            # Test의 경로로 이동한다.\n",
        "            # PATH = root + '/' + 'test'\n",
        "            # 나중에 진짜 데이터를 다룰 때 사용한다.\n",
        "            annot = json_file[idx]\n",
        "\n",
        "            ##### 이 부분은 추후에 작성하기로 하자.\n",
        "            with open(annot, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "\n",
        "            image_name = annot['images'][0]['file_name']\n",
        "            dir = image_name[:2]\n",
        "\n",
        "            image_PATH = PATH + '/' + dir + '/' + image_name\n",
        "            image = Image.open(image_PATH).convert('RGB')\n",
        "            target = {}\n",
        "\n",
        "            target['image_id'] = image_name\n",
        "\n",
        "            return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypiLsatqlf2G"
      },
      "source": [
        "root = '/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data'\n",
        "train_dataset = CustomDataset(root = root,json_file = train_json_list, mode = 'train')\n",
        "test_dataset = CustomDataset(root = root, json_file = test_json_list, mode = 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxSEsZVTOeu"
      },
      "source": [
        "## Pytorch Tutorial을 가지고 분석을 진행해 보기\n",
        "\n",
        "https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html\n",
        "\n",
        "PET를 먼저 진행해 보고 추후에 나머지도 진행해 볼 예정입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Sample_data/train/annotation/PP/PP_050_1520.json', 'r') as f:\n",
        "    data = json.loads(f.read())\n",
        "\n",
        "print(data['images'][0]['file_name'].split('_')[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lundwtUhWjj7",
        "outputId": "cd3246ca-c455-42f2-9516-a37a42a32d05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4TgIuqmTNmX"
      },
      "source": [
        "import torch\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.categories = {1 : 'pet', 2 : 'ps', 3 : 'pp', 4 : 'pe'}\n",
        "\n",
        "        mode = self.root.split('/')[-1]\n",
        "        self.mode = mode\n",
        "\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, 'image', 'PET'))))\n",
        "        if self.mode == 'train':\n",
        "            self.annot = list(sorted(os.listdir(os.path.join(root, 'annotation', 'PET'))))\n",
        "        else:\n",
        "            self.annot = list(sorted(os.listdir(os.path.join(root, 'annotations', 'PET'))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train':\n",
        "            img_path = os.path.join(self.root, 'image','PET', self.imgs[idx])\n",
        "\n",
        "            annot_path = os.path.join(self.root, 'annotation','PET',self.annot[idx])\n",
        "\n",
        "            with open(annot_path, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "            \n",
        "            # 이 부분은 그냥 숫자를 idx를 넣는 부분과 숫자를 넣는 부분이 있고 저는 아무것도 안할 것 이기 때문에 데이터에 그냥 뽑아온 값입니다~\n",
        "            image_id = int(annot_path.split('/')[-1].split('_')[1])\n",
        "\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # width, height로 0인 이미지 생성하여 segmentation 넣어주기\n",
        "            x = annot['images'][0]['width']\n",
        "            y = annot['images'][0]['height']\n",
        "\n",
        "            #각 마스크를 target으로 사용해야함\n",
        "            annot = annot['annotations']\n",
        "\n",
        "            # area, iscrowd는 채울 방법이 없어서 그냥 bbox를 채워넣기로 하자.\n",
        "            mask = []\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            iscrowd = []\n",
        "            area = []\n",
        "            masked_image = np.zeros((x, y), dtype = np.uint8)\n",
        "\n",
        "            for i in range(len(annot)):\n",
        "                segmentation = annot[i]['segmentation'][0]\n",
        "                areas = annot[i]['area']\n",
        "                iscrowds = annot[i]['iscrowd']\n",
        "                xmin, ymin, width, height = annot[i]['bbox'][0], annot[i]['bbox'][1], annot[i]['bbox'][2], annot[i]['bbox'][3]\n",
        "                xmax = xmin + width\n",
        "                ymax = ymin + height\n",
        "\n",
        "                label = annot[i]['category_id']\n",
        "\n",
        "                all_points_x = []\n",
        "                all_points_y = []\n",
        "\n",
        "                for j in range(len(segmentation)):\n",
        "                    if j%2 == 0:\n",
        "                        all_points_x.append(segmentation[j])\n",
        "                    else:\n",
        "                        all_points_y.append(segmentation[j])\n",
        "\n",
        "                polygon_xy = np.array([(x,y) for (x,y) in zip(all_points_x, all_points_y)])\n",
        "\n",
        "                cv2.fillPoly(masked_image, np.uint([polygon_xy]), i+1)\n",
        "\n",
        "                # mask.append(masked_image)\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(label)\n",
        "                area.append(areas)\n",
        "                iscrowd.append(iscrowds)\n",
        "\n",
        "            mask = np.array(masked_image)\n",
        "            obj_ids = np.unique(mask)\n",
        "            obj_ids = obj_ids[1:]\n",
        "            masks = mask == obj_ids[:, None, None]\n",
        "            num_objs = len(obj_ids)\n",
        "            \n",
        "\n",
        "            boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
        "            labels = torch.ones((num_objs,), dtype = torch.int64)\n",
        "            mask = torch.as_tensor(masks, dtype = torch.uint8)\n",
        "            image_id = torch.tensor([image_id])\n",
        "            area = torch.as_tensor(area, dtype = torch.float32)\n",
        "            iscrowd = torch.as_tensor(iscrowd, dtype = torch.int32)\n",
        "\n",
        "            target = {}\n",
        "            target['boxes'] = boxes\n",
        "            target['labels'] = labels\n",
        "            target['masks'] = mask\n",
        "            target['image_id'] = image_id\n",
        "            target['area'] = area\n",
        "            target['iscrowd'] = iscrowd\n",
        "\n",
        "            img = self.preprocess(img)\n",
        "\n",
        "            return img, target\n",
        "            \n",
        "        if self.mode == 'test':\n",
        "            img_path = os.path.join(self.root, 'image','PET', self.imgs[idx])\n",
        "\n",
        "            annot_path = os.path.join(self.root, 'annotations','PET',self.annot[idx])\n",
        "\n",
        "            with open(annot_path, 'r') as f:\n",
        "                annot = json.loads(f.read())\n",
        "            \n",
        "            # 이 부분은 그냥 숫자를 idx를 넣는 부분과 숫자를 넣는 부분이 있고 저는 아무것도 안할 것 이기 때문에 데이터에 그냥 뽑아온 값입니다~\n",
        "            image_id = int(annot_path.split('/')[-1].split('_')[1])\n",
        "\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            # width, height로 0인 이미지 생성하여 segmentation 넣어주기\n",
        "            x = annot['images'][0]['width']\n",
        "            y = annot['images'][0]['height']\n",
        "\n",
        "            #각 마스크를 target으로 사용해야함\n",
        "            annot = annot['annotations']\n",
        "\n",
        "            # area, iscrowd는 채울 방법이 없어서 그냥 bbox를 채워넣기로 하자.\n",
        "            mask = []\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            iscrowd = []\n",
        "            area = []\n",
        "            masked_image = np.zeros((x, y), dtype = np.uint8)\n",
        "\n",
        "            for i in range(len(annot)):\n",
        "                segmentation = annot[i]['segmentation'][0]\n",
        "                areas = annot[i]['area']\n",
        "                iscrowds = annot[i]['iscrowd']\n",
        "                xmin, ymin, width, height = annot[i]['bbox'][0], annot[i]['bbox'][1], annot[i]['bbox'][2], annot[i]['bbox'][3]\n",
        "                xmax = xmin + width\n",
        "                ymax = ymin + height\n",
        "\n",
        "                label = annot[i]['category_id']\n",
        "\n",
        "                all_points_x = []\n",
        "                all_points_y = []\n",
        "\n",
        "                for j in range(len(segmentation)):\n",
        "                    if j%2 == 0:\n",
        "                        all_points_x.append(segmentation[j])\n",
        "                    else:\n",
        "                        all_points_y.append(segmentation[j])\n",
        "\n",
        "                polygon_xy = np.array([(x,y) for (x,y) in zip(all_points_x, all_points_y)])\n",
        "\n",
        "                cv2.fillPoly(masked_image, np.uint([polygon_xy]), i+1)\n",
        "\n",
        "                # mask.append(masked_image)\n",
        "                boxes.append([xmin, ymin, xmax, ymax])\n",
        "                labels.append(label)\n",
        "                area.append(areas)\n",
        "                iscrowd.append(iscrowds)\n",
        "\n",
        "            # masked_image = np.zeros((x,y), dtype = np.uint8)\n",
        "\n",
        "\n",
        "            mask = np.array(masked_image)\n",
        "            obj_ids = np.unique(mask)\n",
        "            obj_ids = obj_ids[1:]\n",
        "            masks = mask == obj_ids[:, None, None]\n",
        "            num_objs = len(obj_ids)\n",
        "            \n",
        "\n",
        "            boxes = torch.as_tensor(boxes, dtype = torch.float32)\n",
        "            labels = torch.ones((num_objs,), dtype = torch.int64)\n",
        "            mask = torch.as_tensor(masks, dtype = torch.uint8)\n",
        "            image_id = torch.tensor([image_id])\n",
        "            area = torch.as_tensor(area, dtype = torch.float32)\n",
        "            iscrowd = torch.as_tensor(iscrowd, dtype = torch.int32)\n",
        "\n",
        "            target = {}\n",
        "            target['boxes'] = boxes\n",
        "            target['labels'] = labels\n",
        "            target['masks'] = mask\n",
        "            target['image_id'] = image_id\n",
        "            target['area'] = area\n",
        "            target['iscrowd'] = iscrowd\n",
        "\n",
        "            img = self.preprocess(img)\n",
        "\n",
        "            return img, target\n",
        "\n",
        "\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        image = img\n",
        "        import torchvision.transforms as T\n",
        "        m, s = np.mean(image, axis = (0, 1)), np.std(image, axis = (0, 1))        \n",
        "        if self.mode == 'train':\n",
        "            transform = T.Compose([\n",
        "                                   T.ToTensor(),\n",
        "                                   T.Normalize(mean = m, std = s),\n",
        "            ])\n",
        "            image = transform(image)\n",
        "        else:\n",
        "            transform = T.Compose([\n",
        "                                #    T.Resize(256),\n",
        "                                   T.ToTensor(),\n",
        "                                   T.Normalize(mean = m, std = s),\n",
        "            ])\n",
        "            image = transform(image)\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvoXdQtCCEzH"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "\n",
        "num_classes = 1 + 1 # 원래 클래스 + 1\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n",
        "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n",
        "# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n",
        "# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n",
        "# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n",
        "\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n",
        "# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n",
        "# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n",
        "# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# 조각들을 Faster RCNN 모델로 합칩니다.\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=2,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "metadata": {
        "id": "-QDlR2nwSs48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "DkqNzf-ESzNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.8.2\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5krPiMPeTbMi",
        "outputId": "f29655d5-0153-4f90-a748-0ecd78ddccda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "HEAD is now at 2f40a483 [v0.8.X] .circleci: Add Python 3.9 to CI (#3063)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "# from torchvision import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # transforms.append(T.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)))\n",
        "    transforms.append(T.ToTensor())\n",
        "    # transforms.append(T.Resize(256))\n",
        "\n",
        "    # transforms.append(T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "    if train:\n",
        "        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "7AKNiQ2xS1mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "dataset = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/train', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "# For Training\n",
        "images,targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "\n",
        "output = model(images,targets)   # Returns losses and detections\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)           # Returns predictions"
      ],
      "metadata": {
        "id": "1muiZL-cS3HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.preprocessing.image import array_to_img\n",
        "# from torchvision.transforms.functional import to_pil_image\n",
        "# from torchvision import transforms\n",
        "\n",
        "# data = to_pil_image(targets[0]['masks'])\n",
        "# data.putpalette([\n",
        "#     0, 0, 0, # black background\n",
        "#     255, 0, 0, # index 1 is red\n",
        "#     255, 255, 0, # index 2 is yellow\n",
        "#     255, 153, 0, # index 3 is orange\n",
        "# ])\n",
        "\n",
        "# data\n",
        "# # array_to_img"
      ],
      "metadata": {
        "id": "wejmmfxgNsMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/train', get_transform(train=True))\n",
        "dataset_test = CustomDataset('/content/drive/MyDrive/NIA 폐플라스틱 객체 검출 예측/Data/test', get_transform(train=False))\n",
        "\n",
        "# # split the dataset in train and test set\n",
        "# torch.manual_seed(1)\n",
        "# indices = torch.randperm(len(dataset)).tolist()\n",
        "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=2, shuffle=False, num_workers=1,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "metadata": {
        "id": "5Jv_6FdUTnGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRwhQL2pjKkK",
        "outputId": "aafea4f7-1152-43ec-bdd9-f5fed44c6ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MaskRCNN(\n",
            "  (transform): GeneralizedRCNNTransform(\n",
            "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
            "  )\n",
            "  (backbone): BackboneWithFPN(\n",
            "    (body): IntermediateLayerGetter(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (fpn): FeaturePyramidNetwork(\n",
            "      (inner_blocks): ModuleList(\n",
            "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      )\n",
            "      (layer_blocks): ModuleList(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      )\n",
            "      (extra_blocks): LastLevelMaxPool()\n",
            "    )\n",
            "  )\n",
            "  (rpn): RegionProposalNetwork(\n",
            "    (anchor_generator): AnchorGenerator()\n",
            "    (head): RPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): RoIHeads(\n",
            "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
            "    (box_head): TwoMLPHead(\n",
            "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "    )\n",
            "    (box_predictor): FastRCNNPredictor(\n",
            "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
            "    )\n",
            "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
            "    (mask_head): MaskRCNNHeads(\n",
            "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu2): ReLU(inplace=True)\n",
            "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu3): ReLU(inplace=True)\n",
            "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (relu4): ReLU(inplace=True)\n",
            "    )\n",
            "    (mask_predictor): MaskRCNNPredictor(\n",
            "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (mask_fcn_logits): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.0005\n",
        "                            , weight_decay=0.005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3, gamma=0.1)"
      ],
      "metadata": {
        "id": "iidKJ_43UDzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets[0]['masks'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcYXidweYE2w",
        "outputId": "cc83b0e9-a2c1-4cb7-c96c-86b1491e96b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2048, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RVIbdpHfEv4",
        "outputId": "5a5d4530-5d7a-4c28-aa39-3c3167559522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2048, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터는 제대로 들어가 있다.\n",
        "print('train의 길이 :', len(data_loader.dataset))\n",
        "print('test의 길이 :', len(data_loader_test.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfyf8VT_sXw_",
        "outputId": "670e36a4-7084-49ad-f419-f1a9e7f61b40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train의 길이 : 1000\n",
            "test의 길이 : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's train it for 10 epochs\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)# let's train it for 10 epochs"
      ],
      "metadata": {
        "id": "lcLBMHIXURWG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "outputId": "59bf3e52-0131-47d5-a916-48f97d658ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/500]  eta: 0:22:43  lr: 0.000002  loss: 9.8513 (9.8513)  loss_classifier: 1.5394 (1.5394)  loss_box_reg: 0.0235 (0.0235)  loss_mask: 7.7635 (7.7635)  loss_objectness: 0.4909 (0.4909)  loss_rpn_box_reg: 0.0340 (0.0340)  time: 2.7270  data: 2.5280  max mem: 6058\n",
            "Epoch: [0]  [ 10/500]  eta: 0:18:18  lr: 0.000012  loss: 8.8286 (8.5626)  loss_classifier: 1.5257 (1.4683)  loss_box_reg: 0.0344 (0.0395)  loss_mask: 6.3219 (6.3917)  loss_objectness: 0.6586 (0.6324)  loss_rpn_box_reg: 0.0257 (0.0306)  time: 2.2412  data: 2.0564  max mem: 6058\n",
            "Epoch: [0]  [ 20/500]  eta: 0:16:42  lr: 0.000022  loss: 6.5908 (6.8519)  loss_classifier: 1.1677 (1.1665)  loss_box_reg: 0.0289 (0.0350)  loss_mask: 4.9172 (5.1031)  loss_objectness: 0.4785 (0.5215)  loss_rpn_box_reg: 0.0209 (0.0258)  time: 2.0572  data: 1.8746  max mem: 6058\n",
            "Epoch: [0]  [ 30/500]  eta: 0:16:00  lr: 0.000032  loss: 3.9533 (5.7445)  loss_classifier: 0.5607 (0.9417)  loss_box_reg: 0.0259 (0.0364)  loss_mask: 2.8758 (4.2606)  loss_objectness: 0.3743 (0.4806)  loss_rpn_box_reg: 0.0209 (0.0252)  time: 1.9346  data: 1.7528  max mem: 6058\n",
            "Epoch: [0]  [ 40/500]  eta: 0:15:37  lr: 0.000042  loss: 2.5057 (4.8844)  loss_classifier: 0.4014 (0.7893)  loss_box_reg: 0.0305 (0.0384)  loss_mask: 1.6672 (3.6027)  loss_objectness: 0.2955 (0.4302)  loss_rpn_box_reg: 0.0209 (0.0238)  time: 1.9864  data: 1.8040  max mem: 6058\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-266-92cccbe56d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    207\u001b[0m             ])\n\u001b[1;32m    208\u001b[0m         \u001b[0mMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# If the exception takes multiple arguments, don't try to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught JSONDecodeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-245-ba0ebe72ac3d>\", line 24, in __getitem__\n    annot = json.loads(f.read())\n  File \"/usr/lib/python3.7/json/__init__.py\", line 348, in loads\n    return _default_decoder.decode(s)\n  File \"/usr/lib/python3.7/json/decoder.py\", line 337, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n  File \"/usr/lib/python3.7/json/decoder.py\", line 353, in raw_decode\n    obj, end = self.scan_once(s, idx)\njson.decoder.JSONDecodeError: Expecting ',' delimiter: line 134 column 22 (char 3193)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[0]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "metadata": {
        "id": "x8KuoepiGdw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction[0]['scores']"
      ],
      "metadata": {
        "id": "hRhoTPokdqHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(prediction[0])):\n",
        "    if prediction[0]['scores'][i] >= 0.113:\n",
        "        plt.imshow(to_pil_image(prediction[0]['masks'][i]))\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "iUzsx3WLHL_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hagH0mTDPVvn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}